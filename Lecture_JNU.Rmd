---
title: "도시계량경제분석 특강"
subtitle: "Modern analysis methods in social science"
author: 
  - "Taehyuk, Kwon Ph.D."
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, default-fonts, xaringan-title.css]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
header-includes:
 \usepackage{kotex}
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(knitr.table.format = "latex")
# install.packages("xaringan")
# install.packages("latex2exp")
# install.packages("showtext")
library(ggplot2)
library(useful)
library(latex2exp)
library(xaringan)
library(dplyr)
library(knitr)
library(kableExtra)
library(caret)
library(ROCR)
library(e1071)
library(pROC)
library(rpart)
library(rpart.plot)
library(xgboost)
```


```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#1381B0",
  secondary_color = "#FF961C",
  inverse_header_color = "#FFFFFF"
)
```


## 정책연구자의 데이터 분석

+ 질적분석의 한계: 이제는 문헌검토 연구조차 질적분석이 쉽지 않음

+ 정책의 실현은 결국 설득의 과정 $\rightarrow$ 표, 그래프 등 숫자로 보여주는게 가장 효과적

+ 세상 모든것은 trend!!, 연구도 trend... 특별한 finding 없이 머신러닝 방법론 몇개의 비교만으로도 논문 게재 사례

+ 소규모 데이터를 통한 정교한 분석 + 질적 보완  $\rightarrow$ 대량의 데이터를 통한 데이터 기반 인사이트 도출, 현실 적용 강조


### 개인적으로는...

+ 정책이론적 내공의 한계, 하던 것이 편하다..

### 여러분은... 내가왜 이런것까지...
+ 원래 통계학은 사회과학도들이..

+ Karl Pearson(정치학 박사), J.M. Keynes(철학박사, 박사논문은 확률론...)

---
## 데이터 과학 vs 데이터 분석  

### **데이터 과학(Data Science) $\ne$ 데이터 분석(Data Analysis) ?**  

#### 1. 데이터 과학(Data Science)?
+ 분석기획, 데이터 획득, 데이터 정제(wrangling), 데이터 분석, 결과해석, 인사이트 도출 등등...  

   $\rightarrow$ **Machine learning**, Programming Skill(R/Python), Statistics,
  DB engineering(Optional), etc...


#### 2. 데이터 분석(Data Analysis)?
+ 분석기획, 데이터 획득, 데이터 정제(wrangling), 데이터 분석, 결과해석, 인사이트 도출 등등...   

  $\rightarrow$ **Data mining**, analysis tools, Statistics, etc..
  
  ###  $\Rightarrow$  `상당부분 중복, 관심대상의 차이`
---


## Machine Learning VS Data Mining

> ### `$$Linear \; Regression$$`    

.pull-left[
#### 1. Data mining: 데이터에서 정보를 캐내는 것
![local image](datamining.png)  
+ 연관: 장바구니 분석 ex. 월마트 기저귀 vs 맥주

+ 회귀: 회귀분석(설명변수 $\rightarrow$ 반응변수) / 예측

+ 분류: 반응변수 by 독립변수  ex. 카드사기(Fraud) 등
]

.pull-right[
#### 2. Machine Learning: Hot!... AI?
+ 지도(Supervised) 학습: 이미 답을 알고 있음  

  + 질병진단: 기존 검사자의 질병유무(환자 필수!)

     + 유전체정보, 생체데이터, X-ray 등등... 
     
  $\Rightarrow$ **회귀분석, Tree계열**, 이미지분류(개/고양이)   

+ 비지도(Unsupervised) 학습: 답모름
  + 데이터는 많은데...(개/고양이 분류 or 돼지 찾기)  

  $\Rightarrow$ Clustering, PCA   

+ *강화(Reinforcement) 학습*: 목표지향(행동/상태변화)
]  
---

## So... what's different?  
.pull-left[### - 관점의 차이
  ### Statistics vs Computer Science
  
  + Why? vs What?
  
  + 왜 그렇게 되는가? vs 무엇이 예상되는가?
]

.pull-right[### - 목적의 차이

 ### Research vs Business
  
  + Inference vs Prediction  
  
  + 인사이트 vs 예측 정확도 
]    

<br>

#### ex) 서울의 지역별 강력범죄 데이터: 강력범죄발생율(y), 구역별 평균소득, 학력수준, 면적당 cctv수, 경범죄 발생율(x) 등등...  
  

.pull-left[#### 강력범죄 발생율에 영향을 미치는 요인
  - cctv수가 면적당 1개 많을때 강력범죄 발생율이~
]

.pull-right[#### 지역의 향후 강력범죄 예측  
  - ~~ 지역의 강력범죄 발생확률이 99%~
]

---

## 대표적인 Machine Learing(?) 기법  
.pull-left[ ### 1. Linear Regression  
 + 가장 기본적인 기법  
 
 + 해석이 쉬움: X $\rightarrow$ Y / 다양한 활용
 
 + 반응변수(y)가 continuous

```{r echo=FALSE, fig.align="left", message=FALSE, warning=F, out.width=370, out.height=280}
mydata <- read.csv("https://www.theissaclee.com/ko/courses/rstat101/examscore2.csv", header=TRUE)
ggplot(data=mydata, aes(x=midterm, y=final)) + 
  geom_point() +
  geom_smooth(method=lm,  se = FALSE) +
  labs(title = "성적분포(중간 vs. 기말성적)", 
       x="중간고사", y="기말고사") +
    annotate("text", x=75, y=60, size=7,
             label = TeX("$y= \\hat{\\beta}_{0}+\\hat{\\beta}_{1}x$")) + 
  coord_fixed()
```
] 


.pull-right[### 2. Logistic Regression
 
 + 머신러닝의 기본, 가장 popular한 분석기법 중 하나
 
 + 반응변수(y)가 binary(or Multinomial...?)
 
 + 해석이 용이, 실제 대부분의 상태전이를 나타냄  
 
```{r echo=FALSE, fig.align="center", message=FALSE, warning=F, out.width=400, out.height=270}

nofcctv <- rnorm(30, 15, 2) 
nofcctv <- sort(nofcctv)
Crocurr <- c(0,0,0,0,0,0,0,1,0,0,1,0,1,1,0,1,0,0,1,1,0,1,1,1,0,1,1,1,1,1)
dat <- as.data.frame(cbind(nofcctv, Crocurr))

ggplot(data = dat, aes(x=nofcctv, y=Crocurr)) +
  geom_point() + 
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))+
  labs(title = "CCTV수와 범죄발생률", 
       x="Number of CCTV", y="Crime Occurence Rate") +
    annotate("text", x=15, y=0.5, size=7,
             label = TeX("$log(\\frac{p}{1-p})= \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x$"))
```
]

---

### 3. Decision Tree(1984, a.k.a. CART)

+ 통계적 수식이 아닌 알고리즘 방법, 최신 ML 알고리즘의 기반 모형

+ x와 y의 관계가 아닌 스무고개로 편가르기: 시각화 기반의 직관적인 결과물, White Box

+ 변수의 중요도에 따라 선분류 기준으로 활용하고, 노드의 purity, 모형의 복잡성을 기준으로 최종모형 결정

+ 분리, 회귀 모두 가능, interaction 허용, 해석 용이, 정규화, 결측치 처리x, 가정 최소화, CV를 통한 Pruning 

+ 단점: Tree의 모형이 자주바뀜(unstable)  $\rightarrow$ bagging(bootstrap & aggregation)을 통해 안정화(변동성 $\downarrow$)
```{r echo=FALSE, fig.align='center', message=FALSE, warning=F, out.width=900, out.height=340}
knitr::include_graphics("dt.jpeg")
```
---

### 4. Random Forest

+ Tree기법의 하나: Many trees $\rightarrow$ Forest, Random에 주목

+ DT의 bagging과 마찬가지로 반복적인 추출을 통한 다수의 Tree를 생성한 매 Tree의 생성시 변수를 random하게 선택

+ Bagging보다 좋은성능과 안정성, 앞선 방법대비 $좋은 예측력^{*}$을 가진다고 알려져 있음

 \* 의학분야는 로지스틱(long et al, 1993) vs 마케팅(고객세분화 등) random forest(Coussement et al., 2014) 

+ Tree모델과 Bagging의 최종진화, $앙상블(Ensemble) 모델^{*}$의 기본, Black box

 \* 정확한 예측을 여러 모형(다수의 분류기, classifier)을 결합하는 방법, 집단지성 활용
 
 
### 5. Gradient Boosting

+ Decision Tree 기반의 Ensemble 방법, XGboost, LightGBM, Cat boost 등 가장 hot한 최신기법의 기본 모형

+ Random Forest보다 일반적으로 성능 좋음(Kaggle Top ranker의 주요기법)

+ (1) 초기 예측값(보통 평균)을 기준으로 오차를 구하고 이를 예측하는 Tree를 생성 (2)기존 예측값 + 오차 $\times$ 학습률 $\rightarrow$ 예측값 update, (3) (1),(2) 과정을 반복하여 최종적으로 개선된(더 이상 오차가 작아지지 않는) 모형 제시

ex) 아파트 단지별 가격 예측: 지하철 역수, 학교수, 범죄율 등 $\rightarrow$ (초기값)평균 가격, 단지별 가격오차 예측 tree 생성...  

---

### 6. Clustering

+ unsupervised learning: 사전에 '정답'없이 데이터를 기반으로 학습

+ 같은 도시내 인접한 지역들은 처음에는 공통점을 인지하지만, 시간이 지나면  지역들이라도 차이점이 보임

+ 즉, 다수의 측정변수 $(x_{1}, x_{2}, \dots, x_{n})$ 를 통해 개체들을 유사성에 따라 몇개의 유형으로 묶어내는 방법

+ 개체 간의 거리에 기반한 clustering $\rightarrow$ '가까우면' 같은 cluster, '멀면' 다른 cluster

+ 거리측정방법: $d(x_{i}, x_{i'}) = \sqrt{\sum_{j=1}^{p} (x_{ij}-x_{i'j})^{2}})$, *Euclidean distance*(변수 표준화)

+ K-means Clustering: n개의 개체를 묶는 k개의 그룹으로 clustering하는 알고리즘  
 1. 사전에 k를 설정하고 n개의 개체 중 k개를 추출하여 그룹의 초기중심으로 설정
 2. 모든개체에 대해 가장 가까운 중심으로 해쳐모여(k개 그룹)
 3. 그룹별 평균으로 그룹중심 update
 4. 새 중심 $\simeq$ 이전중심 $\rightarrow$ 종료 or 2.부터 다시

+ 위 알고리즘의 목표는 각 그룹 $h$ 의 중심 $(m_{h})$과 그룹에 속한 개체들 $(G_{h})$간의 거리의 총합이 최소화 되는 것 즉, $$minimize \sum_{h=1}^{k} \sum_{\in G_{h}}d^{2}(x_{i},m_{h})$$
---

## R을 이용한 데이터 분석

### What is R?

+ 데이터 분석 관점에서 가장 널리 쓰이는 분석 tool이자 언어

+ SAS나 SPSS와 달리 오픈소스로 **무료!** 벨연구에서 개발한 S언어로 출발하여 ~~

+ 일반적으로 Rstudio라는 IDE(Integrated Development Environment, 통합개발환경)를 통해 활용

### Why R?

+ 사회과학(경제, 정책 등등), 자연과학을 총망라하여 거의 모든 통계적 데이터 분석방법이 패키지로 구현되어 있음.

+ Visualization이 뛰어나며, 데이터 전처리, 분석, Business Intelligence, 문서화 등 전과정이 가능

+ 패키지를 활용하되 유연하게 활용가능, 본인의 데이터와 목적에 맞는 코드작성으로 실험적 분석 가능

+ 그러나... 진입장벽이 존재(코딩), 통계학적 지식과 변수속성의 이해를 위한 기본적인 선형대수 등의 사전지식  

---

### 사회과학 연구자가 왜?


+ 사회과학연구란 **사회 현상에 대한 합리적이고 논리적인 설명이자 지식생산 활동**

+ 과학적 지식은 내가 생산한 것을 동일한 조건과 규칙하에서 동일한 결과를 생산가능해야 함
 
+ 분석 결과를 신뢰할 수 있는가? TIER(Teaching Integrity in Empirical Research, 실증연구윤리)

+ R은 정량적인 분석을 기반으로 한 재현가능한 $^{*}$ 과학적 연구에 최적화(R markdown, Git)

+ 그러나 결국 (익숙해지면) 편리함.

> *ex) 일반적인 정량연구 과정: Research Question $\rightarrow$ 주제설정, 문헌검토, 데이터 수집 $\rightarrow$ 데이터 정리 $\rightarrow$ 데이터 분석 $\rightarrow$ 결과물 복사 $\rightarrow$ 한글, 워드로 붙혀넣기 $\rightarrow$ 예쁘게 표/그림으로 정리, 해석 $\rightarrow$ 논문투고, 심사 $\rightarrow$ 게재*


+ 분석 따로, 글쓰기 따로, 밤새서 정리, 투고직전 or 심사과정에서 하나 바뀌면 위의 일부과정 무한반복

   $\Rightarrow$ R은 모든 것을 하나의 환경에서 가능토록 함, ex) this doc
   
+ Github과 연동하여 작성하고 있는 문서의 버전관리에 용이

+ 공동논문 등과 같이 협업이 필요한 경우 서로가 수행한 작업들이 매 버전별로 관리
---


#### * 이런거 진짜 안해도...

```{r echo=FALSE, fig.align='center', message=FALSE, warning=F, out.width=500, out.height=500}
knitr::include_graphics("docversion.jpg")
```
---


### 분석예제

### 1. Logistic Regression

+ 2010 뉴욕주 ACS(American Community Survey) data의 일부: 가계수입의 영향요인

+ $150,000 이상 True, 미만 False

#### 데이터 불러오기, 확인  

```{r echo=FALSE, message=FALSE, warning=FALSE}
acs <- read.table("http://jaredlander.com/data/acs_ny.csv", sep=",", header = TRUE,
                  stringsAsFactors = FALSE)
acs$Income <- with(acs, FamilyIncome >= 150000)
acs %>% 
  sample_n(5) %>% 
  select(Income, FamilyIncome, HouseCosts, NumWorkers, OwnRent, NumBedrooms,
         FamilyType, HeatingFuel, Language) %>% kbl() %>% kable_paper("hover", full_width = F)
```
---

#### 소득의 분포는? $\rightarrow$ EDA(Exploratory Data Anaysis)

.pull-left[

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot(acs,aes(x=FamilyIncome, fill=FamilyType)) +
    geom_density() +
    geom_vline(xintercept = 150000) + 
    scale_x_continuous(labels = multiple.dollar, limits = c(0,1000000))
```
]

.pull-right[ + 전체적인 소득분포와, $1500만 이상인 경우 대략적인 위치를 확인할 수 있음

+ 이외에도 다양한 형태의 그래프와 표를 통해 설명변수에 따른 소득분포 등을 확인  

+ 실제 데이터의 정리와 EDA과정에서의 visualization을 통한 시사점  
  + ex) FamilyType: 결혼가정의 소득이 높은 경향.. 지원정책은?

+ 확인된 특성들을 기반으로 변수의 변환, 모형에 투입할 변수 등을 선택: Domain knowledge

+ 기타 다양한 데이터의 특성을 바탕으로 연구 가설검정에 필요한 분석계획 설정

+ 실제 연구에서는 데이터를 먼저 파악해서 연구아이디어를 얻는 경우도..
]

---

#### logistic regression 실습
.pull-left[ 

+ 모형의 설정    

```{r echo=TRUE, message=FALSE, warning=FALSE}
income1 <- glm(Income ~ HouseCosts +
               NumWorkers + OwnRent + 
               NumBedrooms + FamilyType, 
               data=acs, 
               family=binomial("logit"))
```

```{r echo=TRUE, message=FALSE,, eval=F}

summary(income1)
```
]   

.pull-right[

+ 분석결과(Logistic Regression result)

```{r echo=FALSE, message=FALSE, warning=FALSE, results ='asis', out.width=400, out.height=500,  class.source='regression'}
stargazer::stargazer(income1, type="html", style="apsr",
                    header = FALSE, single.row = TRUE, omit.stat = c("ll","AIC"),
                    no.space = TRUE, column.sep.width = "-15pt", 
                    font.size = "tiny")
```
]

---

#### 조금 더 machine learning 스럽게 하면...

.pull-left[ 

```{r echo=TRUE, message=FALSE, warning=FALSE}
set.seed(1000)
test_idx <- createDataPartition(acs$Income, p=0.7, list = FALSE)
train.acs <- acs[test_idx,]
test.acs <- acs[-test_idx,]
ml.glm <- glm(Income ~ HouseCosts +
               NumWorkers + OwnRent + 
               NumBedrooms + FamilyType, 
               data=train.acs, 
               family=binomial("logit"))
ml.glm.predicted <- predict(ml.glm, newdata=test.acs, type="response")
```

```{r echo=TRUE, message=FALSE, warning=FALSE, eval=F}
confusionMatrix(data = as.factor(ml.glm.predicted>0.5), reference = as.factor(test.acs$Income)) 
```

|       |       |*Actual* |
|-------|-------|-------|
|       |       | **FALSE** | **TRUE**  |
|***Predicted***| **FALSE** | 5217  | 1041 |
|         | **TRUE**  |  237  | 328  |

]

.pull-right[ 
#### ※참고 : 분류성능  알아보기

|       |       |*Actual* |
|-------|-------|-------|
|       |       | **FALSE(0)** | **TRUE(1)**  |
|***Predicted***| **FALSE(0)** | True Negative(TN) | False Negative(FN) |
|         | **TRUE(1)**  |  False Positive(FP)  | True Positive(TP)  |

+ $Accuracy(정확도) = \frac{TN+TP}{TN+FN+FP+TP} = 0.8127$  

+ $Sensitivity(민감도) = \frac{TP}{TP+FN} = recall = 0.9565$

+ $Specificity(특이도) = \frac{TN}{FP+TN} = 0.2396$

+ $Precision(정밀도) = \frac{TP}{TP+FP} = 0.5805$

]
---

+ **ROC(Receiver operating characteristic) Curve & AUC(Area Under Curve)**

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align = 'center'}
#define object to plot and calculate AUC

roc_pred <- prediction(prediction = ml.glm.predicted, labels=test.acs$Income)
roc_perf <- performance(roc_pred , "tpr" , "fpr")
auc <- round(auc(test.acs$Income, ml.glm.predicted),4)
plot(roc_perf,
     colorize = TRUE,
     print.cutoffs.at= seq(0,1,0.05),
     text.adj=c(-0.2,1.7), xlab="1-specificity", ylab="sensitvity")
title(main=paste0("ROC Curve", "(Auc=", auc,")"))

```
---

#### Decistion Tree 실습    

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align = 'center', out.width=800, out.height=550}
acsTree <- rpart(Income ~ HouseCosts +
               NumWorkers + OwnRent + 
               NumBedrooms + FamilyType, 
               data=train.acs, control=rpart.control(minsplit=8, cp=0.005))

rpart.plot::rpart.plot(acsTree, type=2, tweak=1.2, gap=0, space=0.5, xcompact=FALSE,
                       ycompact=FALSE)
```
---



#### Decision Tree model evaluation

.pull-left[ 

```{r echo=FALSE, message=FALSE, warning=FALSE}
ml.dt.predicted <- predict(acsTree, test.acs)
```

+ **Confusion Matrix**
```{r echo=FALSE, message=FALSE, warning=FALSE, eval=F}
confusionMatrix(data = as.factor(ml.dt.predicted>0.5), as.factor(test.acs$Income))
```

|       |       |*Actual* |           |
|-------|-------|-------|
|       |       | **FALSE** | **TRUE**  |
|***Predicted***| **FALSE** | 5289  | 1112 |
|         | **TRUE**  |  165  | 257  |  

    
 + 주요 지표
   + Accuracy = 0.8128
  
   + Sensitivity = recall = 0.9697
   
   + Specificity = 0.1877
   
   + precision = 0.6090
]


.pull-right[ 
+ **ROC(Receiver operating characteristic) Curve & AUC(Area Under Curve)**

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align = 'center'}
#define object to plot and calculate AUC

roc_pred_dt <- prediction(prediction = ml.dt.predicted, labels=test.acs$Income)
roc_perf_dt <- performance(roc_pred_dt , "tpr" , "fpr")
auc_dt <- round(auc(test.acs$Income, ml.dt.predicted),4)
plot(roc_perf_dt,
     colorize = TRUE,
     print.cutoffs.at= seq(0,1,0.05),
     text.adj=c(-0.2,1.7), xlab="1-specificity", ylab="sensitvity")
title(main=paste0("ROC Curve", "(Auc=", auc_dt,")"))

```
]

---

#### XGboost(e<u>X</u>treme <u>G</u>radient <u>Boost</u>ing)  

```{r echo=FALSE, message=FALSE, warning=FALSE}

acsBoostModel <- Income ~ HouseCosts + NumWorkers + OwnRent + NumBedrooms + FamilyType - 1
acsX <- build.x(acsBoostModel, data = train.acs, contrasts = FALSE)
acsX.test <- build.x(acsBoostModel, data = test.acs, contrasts = FALSE)
acsY <- build.y(acsBoostModel, data = train.acs)
acsY.test <- build.y(acsBoostModel, data = test.acs)
# acsY <- as.integer(relevel(acsY, ref = "Low"))-1
# acsY.test <- as.integer(relevel(acsY.test, ref = "Low"))-1
```

+ 모형 설정
```{r echo=TRUE, message=FALSE, warning=FALSE, results="hide"}
acsBstMod <- xgboost(data=acsX, label = acsY, max.depth = 4, eta=0.3, nround=20,objective='binary:logistic')
```

```{r echo=FALSE, message=FALSE, warning=FALSE, include=F}
pred.bst <- predict(acsBstMod, acsX.test)
predicted.bst <- as.numeric(pred.bst>0.5)
```

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.align = 'center', out.width=650, out.height=450}
x.importance <- xgb.importance(colnames(acsX), model = acsBstMod)
xgb.plot.importance(x.importance, xlim=c(0,1), main="Family income prediction in ACS")
```
---
**Model evaluation**
.pull-left[ 

+ Confusion Matrix
```{r echo=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
confusionMatrix(data = as.factor(predicted.bst), as.factor(acsY.test))
```

|       |       |*Actual* |           |
|-------|-------|-------|
|       |       | **FALSE** | **TRUE**  |
|***Predicted***| **FALSE** | 5174  | 985 |
|         | **TRUE**  |  280  | 384  |     
</br>
    
     
  + 주요 지표
 
    + Accuracy = 0.8146
    
    + Sensitivity = recall = 0.9487
    
    + Specificity = 0.2805
    
    + precision = 0.5783
]

.pull-right[ + **ROC(Receiver operating characteristic) Curve & AUC(Area Under Curve)**

```{r echo=FALSE, message=FALSE, warning=FALSE}

roc_pred_bst <- prediction(prediction = pred.bst, labels=acsY.test)
roc_perf_bst <- performance(roc_pred_bst , "tpr" , "fpr")
auc_bst <- round(auc(acsY.test, pred.bst),4)
plot(roc_perf_bst,
     colorize = TRUE,
     print.cutoffs.at= seq(0,1,0.05),
     text.adj=c(-0.2,1.7), xlab="1-specificity", ylab="sensitvity")
title(main=paste0("ROC Curve", "(Auc=", auc_bst,")"))

```
]

---

#### 정리하면...

+ Data Science는 Data analysis를 대부분 포함

 + 마찬가지로 Machine learning과 Data Mining도 목적과 관심의 차이
 
 + 분석방법은 다 비슷함. 

+ 그러나 이왕이면, Machine Learning이라고 하자...(연구도 trend!)

+ 다들 아는 기법이지만 선형회귀분석만큼 Powerful하고 널리쓰이는 방법은 없음

  + 대부분의 분석방법은 회귀분석의 개념에서 파생
  
+ 그래도 가장 많이 쓰이는 방법론들 LR, DT, RF, Boost, Clustering 등은 대략적인 용도나 방법을 알아두자

+ 혹시 계량적인 방법론을 통해 연구를 하고자 한다면, R 또는 Python을 익혀두자

  + 활용의 범위가 넓고(데이터 전처리, 분석, Publication, visualization 등등), 유연하며   
  
  + 갈수록 재현가능한 연구결과물의 중요성이 커짐
  + 오픈소스라 공부할 자료가 많고, 무엇보다 <span style="color:blue">**`무료`**</span>! (일반적인 직장에서는 유료 툴 잘 안사줌)
  
+ 정책연구에서는... 아직까지 머신러닝 방법론들의 실무적인 활용이 일반적이지 않음

+ 최근 점차 도입되고 있으나, 핵심은 EDA과정에서 정책분야별 전문성을 통한 유의미한 해석과 결과의 예측
  



 
 
 